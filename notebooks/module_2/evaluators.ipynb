{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high-level, an evaluator judges an invocation of your LLM application against a reference example, and returns an evaluation score.\n",
    "\n",
    "In LangSmith evaluators, we represent this process as a function that takes in a Run (representing the LLM app invocation) and an Example (representing the data point to evaluate), and returns Feedback (representing the evaluator's score of the LLM app invocation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Evaluator](../../images/evaluator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def correct_label(inputs: dict, reference_outputs: dict, outputs: dict) -> dict:\n",
    "  score = outputs.get(\"output\") == reference_outputs.get(\"label\")\n",
    "  return {\"score\": int(score), \"key\": \"correct_label\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-Judge Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference).\n",
    "\n",
    "Here is an example of how you might define an LLM-as-judge evaluator with structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Similarity_Score(BaseModel):\n",
    "    similarity_score: int = Field(description=\"Semantic similarity score between 1 and 10, where 1 means unrelated and 10 means identical.\")\n",
    "\n",
    "# NOTE: This is our evaluator\n",
    "def compare_semantic_similarity(inputs: dict, reference_outputs: dict, outputs: dict):\n",
    "    input_question = inputs[\"question\"]\n",
    "    reference_response = reference_outputs[\"output\"]\n",
    "    run_response = outputs[\"output\"]\n",
    "    \n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {   \n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a semantic similarity evaluator. Compare the meanings of two responses to a question, \"\n",
    "                    \"Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar. \"\n",
    "                    \"Provide a score between 1 and 10, where 1 means completely unrelated, and 10 means identical in meaning.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {input_question}\\n Reference Response: {reference_response}\\n Run Response: {run_response}\"}\n",
    "        ],\n",
    "        response_format=Similarity_Score,\n",
    "    )\n",
    "\n",
    "    similarity_score = completion.choices[0].message.parsed\n",
    "    return {\"score\": similarity_score.similarity_score, \"key\": \"similarity\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out with some fun animal facts! üéâ\n",
    "\n",
    "NOTE: We purposely made this answer wrong, so we expect to see a low similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity score: {'score': 1, 'key': 'similarity'}\n"
     ]
    }
   ],
   "source": [
    "# From Dataset Example - Fun facts about animals! üê®\n",
    "inputs = {\n",
    "  \"question\": \"How many hours can koalas sleep per day?\"\n",
    "}\n",
    "reference_outputs = {\n",
    "  \"output\": \"Koalas can sleep for up to 22 hours a day! They need all that rest because their eucalyptus diet provides very little energy, so they conserve it by sleeping most of the time.\"\n",
    "}\n",
    "\n",
    "# From Run - Purposely wrong answer to test low score\n",
    "outputs = {\n",
    "  \"output\": \"Koalas only sleep about 8 hours a day like humans and are very active animals.\"\n",
    "}\n",
    "\n",
    "similarity_score = compare_semantic_similarity(inputs, reference_outputs, outputs)\n",
    "print(f\"Semantic similarity score: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define evaluators using Run and Example directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "def compare_semantic_similarity_v2(root_run: Run, example: Example):\n",
    "    input_question = example[\"inputs\"][\"question\"]\n",
    "    reference_response = example[\"outputs\"][\"output\"]\n",
    "    run_response = root_run[\"outputs\"][\"output\"]\n",
    "    \n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {   \n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a semantic similarity evaluator. Compare the meanings of two responses to a question, \"\n",
    "                    \"Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar. \"\n",
    "                    \"Provide a score between 1 and 10, where 1 means completely unrelated, and 10 means identical in meaning.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {input_question}\\n Reference Response: {reference_response}\\n Run Response: {run_response}\"}\n",
    "        ],\n",
    "        response_format=Similarity_Score,\n",
    "    )\n",
    "\n",
    "    similarity_score = completion.choices[0].message.parsed\n",
    "    return {\"score\": similarity_score.similarity_score, \"key\": \"similarity\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity score: {'score': 1, 'key': 'similarity'}\n"
     ]
    }
   ],
   "source": [
    "sample_run = {\n",
    "  \"name\": \"Fun Facts Run\",\n",
    "  \"inputs\": {\n",
    "    \"question\": \"How many hearts does an octopus have?\"\n",
    "  },\n",
    "  \"outputs\": {\n",
    "    \"output\": \"An octopus has only one heart, just like humans do.\"\n",
    "  },\n",
    "  \"is_root\": True,\n",
    "  \"status\": \"success\",\n",
    "  \"extra\": {\n",
    "    \"metadata\": {\n",
    "      \"topic\": \"marine_biology\",\n",
    "      \"difficulty\": \"intermediate\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "sample_example = {\n",
    "  \"inputs\": {\n",
    "    \"question\": \"How many hearts does an octopus have?\"\n",
    "  },\n",
    "  \"outputs\": {\n",
    "    \"output\": \"An octopus has three hearts! Two pump blood to the gills, while the third pumps blood to the rest of the body. Interestingly, the main heart stops beating when they swim, which is why they prefer crawling.\"\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"dataset_split\": [\n",
    "      \"animal_facts\",\n",
    "      \"verified\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "similarity_score = compare_semantic_similarity_v2(sample_run, sample_example)\n",
    "print(f\"Semantic similarity score: {similarity_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
