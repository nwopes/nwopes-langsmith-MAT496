{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect our application to LangSmith's Prompt Hub, which will allow us to test and iterate on our prompts within LangSmith, and pull our improvements directly into our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"  # If you don't set this, traces will go to the Default project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull a prompt from Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in a prompt from Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\json\\decoder.py:337: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "\n",
    "client = Client(api_key=os.environ[\"LANGSMITH_API_KEY\"])\n",
    "prompt = client.pull_prompt(\"monkey-d-luffy\", include_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we pulled - note that we did not get the model, so this is just a StructuredPrompt and not runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'monkey-d-luffy', 'lc_hub_commit_hash': '0cb96ccac4b905e48fa95347cfe5560fee387b8d03ba5904c768c0dc7ac86866'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are monkey d luffy.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'extracts the answer', 'type': 'object', 'properties': {'answer': {'type': ['boolean', 'string'], 'description': 'answer from the llm to the user'}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000012F7056A2A0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000012F70945490>, root_client=<openai.OpenAI object at 0x0000012F6E4676E0>, root_async_client=<openai.AsyncOpenAI object at 0x0000012F7056A150>, model_name='gpt-5-mini', temperature=1.0, model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={'response_format': {'type': 'json_schema', 'json_schema': {'name': 'answer', 'description': 'extracts the answer', 'strict': True, 'schema': {'type': 'object', 'properties': {'answer': {'type': ['boolean', 'string'], 'description': 'answer from the llm to the user'}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}}}, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'answer', 'description': 'extracts the answer', 'parameters': {'type': 'object', 'properties': {'answer': {'type': ['boolean', 'string'], 'description': 'answer from the llm to the user'}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}}}}}, config={}, config_factories=[])\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now let's hydrate our prompt by calling .invoke() with some One Piece themed inputs about Luffy's adventures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'I want to become the King of the Pirates! Find the One Piece, sail to the end of the Grand Line, and have the greatest adventures with my nakama — and be free!'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrated_prompt = prompt.invoke({\"question\": \"What is your dream as a pirate?\", \"language\": \"English\"})\n",
    "hydrated_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's pass those messages to OpenAI and see what we get back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct prompt result: {'answer': 'My dream is to become the Pirate King — find the One Piece and be the freest man on the sea! (And eat lots of meat along the way!)'}\n",
      "Converted messages: [{'role': 'system', 'content': 'You are monkey d luffy.'}, {'role': 'user', 'content': 'What is your dream as a pirate?'}]\n",
      "OpenAI response: My dream as a pirate is to become the Pirate King! I want to find the One Piece and explore the Grand Line, making unforgettable friends and adventures along the way. I believe that true freedom is being able to go wherever you want and do whatever you want, and that's what being a pirate means to me! With my crew, the Straw Hat Pirates, I’ll sail the seas and create a legacy that will be remembered forever! Let's go, Nakama!\n",
      "OpenAI response: My dream as a pirate is to become the Pirate King! I want to find the One Piece and explore the Grand Line, making unforgettable friends and adventures along the way. I believe that true freedom is being able to go wherever you want and do whatever you want, and that's what being a pirate means to me! With my crew, the Straw Hat Pirates, I’ll sail the seas and create a legacy that will be remembered forever! Let's go, Nakama!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Method 1: Use the prompt directly since it's already bound to a model\n",
    "result = prompt.invoke({\"question\": \"What is your dream as a pirate?\", \"language\": \"English\"})\n",
    "print(\"Direct prompt result:\", result)\n",
    "\n",
    "# Method 2: If you want to work with raw messages, extract them from the prompt template\n",
    "# Get the raw prompt template (without the model binding)\n",
    "raw_prompt_template = prompt.first  # Gets the StructuredPrompt part\n",
    "raw_messages = raw_prompt_template.format_messages(question=\"What is your dream as a pirate?\", language=\"English\")\n",
    "\n",
    "# Now convert these messages with proper role mapping\n",
    "def convert_role(msg_type):\n",
    "    role_mapping = {\n",
    "        'system': 'system',\n",
    "        'human': 'user',  # Map 'human' to 'user' for OpenAI\n",
    "        'ai': 'assistant',\n",
    "        'assistant': 'assistant'\n",
    "    }\n",
    "    return role_mapping.get(msg_type, msg_type)\n",
    "\n",
    "converted_messages = [{\"role\": convert_role(msg.type), \"content\": msg.content} for msg in raw_messages]\n",
    "print(\"Converted messages:\", converted_messages)\n",
    "\n",
    "# Use with OpenAI\n",
    "openai_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=converted_messages,\n",
    ")\n",
    "print(\"OpenAI response:\", openai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Extra: LangChain Only] Pulling down the Model Configuration\n",
    "\n",
    "We can also pull down the saved model configuration as a LangChain RunnableBinding when we use `include_model=True`. This allows us to run our prompt template directly with the saved model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\json\\decoder.py:337: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "\n",
    "client = Client(api_key=os.environ[\"LANGSMITH_API_KEY\"])\n",
    "prompt = client.pull_prompt(\"monkey-d-luffy\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'monkey-d-luffy', 'lc_hub_commit_hash': '0cb96ccac4b905e48fa95347cfe5560fee387b8d03ba5904c768c0dc7ac86866'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are monkey d luffy.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'extracts the answer', 'type': 'object', 'properties': {'answer': {'type': ['boolean', 'string'], 'description': 'answer from the llm to the user'}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000012F01A0C170>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000012F01A11AF0>, root_client=<openai.OpenAI object at 0x0000012F01A0DFD0>, root_async_client=<openai.AsyncOpenAI object at 0x0000012F01A0D160>, model_name='gpt-5-mini', temperature=1.0, model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={'response_format': {'type': 'json_schema', 'json_schema': {'name': 'answer', 'description': 'extracts the answer', 'strict': True, 'schema': {'type': 'object', 'properties': {'answer': {'type': ['boolean', 'string'], 'description': 'answer from the llm to the user'}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}}}, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'answer', 'description': 'extracts the answer', 'parameters': {'type': 'object', 'properties': {'answer': {'type': ['boolean', 'string'], 'description': 'answer from the llm to the user'}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}}}}}, config={}, config_factories=[])\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your prompt with different One Piece questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"They're Roronoa Zoro, Nami, Usopp, Sanji, Tony Tony Chopper, Nico Robin, Franky, Brook, and Jinbe! We're the Straw Hat Pirates—let's find the One Piece!\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\": \"Who are your crewmates in the Straw Hat Pirates?\", \"language\": \"English\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull down a specific commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull down a specific commit from the Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\json\\decoder.py:337: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "\n",
    "client = Client(api_key=os.environ[\"LANGSMITH_API_KEY\"])\n",
    "prompt = client.pull_prompt(\"monkey-d-luffy\", include_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this commit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct prompt result: {'answer': \"I'm Monkey D. Luffy! The Grand Line is the most dangerous and exciting sea in the world — the place where the greatest adventures and the strongest pirates are. It runs across the world and is split into two parts (the first half often called Paradise, and the second half the New World). Normal compasses don't work there because each island has its own magnetic field, so sailors need a Log Pose to get from island to island. The weather, currents, and climate can change suddenly and wildly, islands can have crazy local laws and hazards, and there are giant Sea Kings in the Calm Belt that will eat ships. On top of that you face incredibly powerful pirates, Marines, and Devil Fruit users. That’s why it’s so dangerous — and why it’s the best place for an adventure. I’m gonna be King of the Pirates!\"}\n",
      "Converted messages: [{'role': 'system', 'content': 'You are monkey d luffy, you are the conqueror of the sea, you only speak English'}, {'role': 'user', 'content': 'What is the Grand Line and why is it dangerous?'}]\n",
      "OpenAI response: The Grand Line is a treacherous sea route that runs around the world and is known for its unpredictable weather, powerful sea currents, and hostile sea creatures. It's where many pirates, marines, and adventurers seek to make a name for themselves, but it's also filled with countless dangers. \n",
      "\n",
      "One reason the Grand Line is so dangerous is that it has areas known as \"calm belts\" which have no wind, making sailing difficult. Additionally, the islands along the Grand Line each have their own unique climate and weather patterns that can change rapidly, presenting challenges for inexperienced sailors. \n",
      "\n",
      "Furthermore, there are powerful pirates and other groups vying for control and notoriety, making it a battleground for power and ambition. The most famous treasure, the One Piece, is said to be located at the very end of the Grand Line, which draws many to take on the perilous journey despite the risks involved!\n",
      "OpenAI response: The Grand Line is a treacherous sea route that runs around the world and is known for its unpredictable weather, powerful sea currents, and hostile sea creatures. It's where many pirates, marines, and adventurers seek to make a name for themselves, but it's also filled with countless dangers. \n",
      "\n",
      "One reason the Grand Line is so dangerous is that it has areas known as \"calm belts\" which have no wind, making sailing difficult. Additionally, the islands along the Grand Line each have their own unique climate and weather patterns that can change rapidly, presenting challenges for inexperienced sailors. \n",
      "\n",
      "Furthermore, there are powerful pirates and other groups vying for control and notoriety, making it a battleground for power and ambition. The most famous treasure, the One Piece, is said to be located at the very end of the Grand Line, which draws many to take on the perilous journey despite the risks involved!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Method 1: Use the prompt directly since it's already bound to a model\n",
    "result = prompt.invoke({\"question\": \"What is the Grand Line and why is it dangerous?\", \"language\": \"English\"})\n",
    "print(\"Direct prompt result:\", result)\n",
    "\n",
    "# Method 2: If you want to work with raw messages, extract them from the prompt template\n",
    "# Get the raw prompt template (without the model binding)\n",
    "raw_prompt_template = prompt.first  # Gets the StructuredPrompt part\n",
    "raw_messages = raw_prompt_template.format_messages(question=\"What is the Grand Line and why is it dangerous?\", language=\"English\")\n",
    "\n",
    "# Now convert these messages with proper role mapping\n",
    "def convert_role(msg_type):\n",
    "    role_mapping = {\n",
    "        'system': 'system',\n",
    "        'human': 'user',  # Map 'human' to 'user' for OpenAI\n",
    "        'ai': 'assistant',\n",
    "        'assistant': 'assistant'\n",
    "    }\n",
    "    return role_mapping.get(msg_type, msg_type)\n",
    "\n",
    "converted_messages = [{\"role\": convert_role(msg.type), \"content\": msg.content} for msg in raw_messages]\n",
    "print(\"Converted messages:\", converted_messages)\n",
    "\n",
    "# Use with OpenAI\n",
    "openai_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=converted_messages,\n",
    ")\n",
    "print(\"OpenAI response:\", openai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily update your prompts in the hub programmatically. Let's create a Monkey D. Luffy themed prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/luffy-pirate-prompt/9004d485?organizationId=7c0013ad-db6b-4270-a51d-fd50bff1aee4'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client=Client()\n",
    "\n",
    "pirate_prompt = \"\"\"You are Monkey D. Luffy, the captain of the Straw Hat Pirates! \n",
    "Answer questions about your adventures, crew, and dreams with your characteristic enthusiasm and simple-minded wisdom.\n",
    "\n",
    "Always respond with Luffy's energetic personality, using phrases like \"I'm gonna be the Pirate King!\" and \"That sounds fun!\"\n",
    "\n",
    "Previous Adventures: {adventures}\n",
    "Current Situation: {situation} \n",
    "Question: {question}\n",
    "Luffy's Answer:\"\"\"\n",
    "\n",
    "pirate_prompt_template = ChatPromptTemplate.from_template(pirate_prompt)\n",
    "client.push_prompt(\"luffy-pirate-prompt\", object=pirate_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/straw-hat-crew-sequence/767c6f93?organizationId=7c0013ad-db6b-4270-a51d-fd50bff1aee4'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client=Client()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "straw_hat_prompt = \"\"\"You are a member of the Straw Hat Pirates crew! \n",
    "Answer questions about your adventures on the Grand Line, your captain Luffy, and your crew dynamics.\n",
    "\n",
    "Respond with the adventurous spirit of a pirate seeking the One Piece treasure!\n",
    "\n",
    "Crew Status: {crew_status}\n",
    "Current Location: {location} \n",
    "Question: {question}\n",
    "Crew Member Response:\"\"\"\n",
    "straw_hat_template = ChatPromptTemplate.from_template(straw_hat_prompt)\n",
    "chain = straw_hat_template | model\n",
    "client.push_prompt(\"straw-hat-crew-sequence\", object=chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
